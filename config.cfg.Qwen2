[OpenAI]
# Optional. Set these if you are testing OpenAI models.
api_key = 1234

# Optional. Set this if you are using an alternative OpenAI compatible endpoint, like ollama running locally
openai_compatible_url = http://localhost:8080/v1/

[Huggingface]
# Optional. This allows the script to download gated models.
access_token =

# Optional. This is where models will be downloaded to.
# - defaults to ~/.cache/huggingface/hub
cache_dir = 

[Results upload]
# Optional. Set this to allow uploading of results to a google sheets spreadsheet.
# Note: this feature requires extra configuration (see README).
google_spreadsheet_url =

[Options]
# Set to true or false
trust_remote_code = false

[Oobabooga config]
# e.g. ~/text-generation-webui/start_linux.sh
ooba_launch_script = ~/text-generation-webui/start_linux.sh

# Specify any additional oobabooga launch params (this can be overridden on a per-model basis).
# e.g.:
# --auto-devices --loader llama.cpp
ooba_params_global = 

# Set to true or false. Setting to "true" only supports linux (and possibly mac).
# If set to false, you must launch ooba yourself with --api flag and load the model yourself before running the benchmark.
# If you are launching ooba yourself, the model_path param will be ignored, and all scheduled ooba benchmark runs will use the same model.
automatically_launch_ooba = true

# Ooba api request timeout in seconds (default 120). Set higher if you are expecting long inference times.
ooba_request_timeout = 120

[Benchmarks to run]
# Define benchmarks in the following format:
# run_id, instruction_template, model_path, lora_path, quantization, n_iterations, inference_engine, ooba_params, downloader_filters

# Details:
#
# - run_id: A name to identify the benchmark run
# - instruction_template: The filename of the instruction template defining the prompt format, minus the .yaml (e.g. Alpaca)
# - model_path: Huggingface model ID, local path, or OpenAI model name
# - lora_path (optional): Path to local lora adapter
# - quantization: Using bitsandbytes package (8bit, 4bit, None)
# - n_iterations: Number of benchmark iterations (final score will be an average)
# - inference_engine: Set this to transformers, openai or ooba.
# - ooba_params (optional): Any additional ooba params for loading this model (overrides the global setting above)
# - downloader_filters (optional): Specify --include or --exclude patterns (using same syntax as huggingface-cli download)

# Examples:
#
# myrun1, openai_api, gpt-3.5-turbo, , , 1, openai, ,
# myrun2, Llama-v2, meta-llama/Llama-2-7b-chat-hf, /path/to/local/lora/adapter, 8bit, 3, transformers, , ,
# myrun3, Alpaca, ~/my_local_model, , None, 1, ooba, --loader transformers --n_ctx 1024 --n-gpu-layers -1, 
# myrun4, Mistral, TheBloke/Mistral-7B-Instruct-v0.2-GGUF, , None, 1, ooba, --loader llama.cpp --n-gpu-layers -1 --tensor_split 1,3,5,7, --include ["*Q3_K_M.gguf", "*.json"]
# myrun5, Mistral, mistralai/Mistral-7B-Instruct-v0.2, , None, 1, ooba, --loader transformers --gpu-memory 12, --exclude "*.bin"
# myrun5, Alpaca, None, , None, 1, llama, None,
# llama-70b, Alpaca, Llama-3-70B-Instruct-exl2, , None, 1, llama, None,
# Miqu_combined_Consistency_33_69,2024-02-21 02:36:32,Mistral,/home/dnhkng/Documents/models/miqu-1-70b-sf-4.0bpw-h6-exl2,/home/dnhkng/Documents/LLM/EQ-Bench/None,none,82.54,v2,171.0,1,ooba,  --loader exllamav2 --franken_start 33 --franken_stop 69 --franken_repeat 1 --max_seq_len 8192 --gpu-split 18,18,,
# Miqu_combined_Consistency_59_74,2024-02-21 03:28:56,Mistral,/home/dnhkng/Documents/models/miqu-1-70b-sf-4.0bpw-h6-exl2,/home/dnhkng/Documents/LLM/EQ-Bench/None,none,81.24,v2,171.0,1,ooba,  --loader exllamav2 --franken_start 59 --franken_stop 74 --franken_repeat 1 --max_seq_len 8192 --gpu-split 18,18,,

# 15_42_bad, Vicuna-v1.1, /home/dnhkng/Documents/models/Nous-Capybara-34B, None, None, 1, ooba,  --loader exllamav2 --franken_start 15 --franken_stop 42 --franken_repeat 1, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-56]-[0-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-22]-[8-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-53]-[11-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-65]-[24-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-33]-[22-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-17]-[15-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-59]-[52-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-11]-[8-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-64]-[53-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-35]-[6-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-65]-[19-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-48]-[32-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-61]-[13-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-28]-[23-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-41]-[15-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-37]-[35-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-54]-[9-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-69]-[65-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-63]-[62-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-10]-[9-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-53]-[37-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-53]-[22-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-40]-[31-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-49]-[47-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-60]-[9-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-64]-[45-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-51]-[28-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-63]-[57-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-56]-[45-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-61]-[42-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-50]-[4-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-68]-[6-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-64]-[9-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-66]-[5-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-49]-[4-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-27]-[0-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-7]-[4-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-25]-[20-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-21]-[16-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-66]-[20-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-49]-[5-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-64]-[24-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-66]-[18-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-54]-[0-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-66]-[37-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-64]-[34-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-41]-[10-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-43]-[10-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-41]-[16-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-53]-[3-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-28]-[22-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-45]-[2-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-12]-[0-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-60]-[40-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-73]-[36-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-41]-[8-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-53]-[25-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-62]-[36-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-20]-[19-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-54]-[50-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-38]-[7-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-44]-[29-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-71]-[60-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-73]-[40-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-54]-[16-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-28]-[14-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-44]-[9-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-45]-[6-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-17]-[5-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-73]-[49-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-64]-[2-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-22]-[14-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-43]-[31-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-33]-[25-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-47]-[39-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-62]-[19-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-59]-[6-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-6]-[1-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-45]-[1-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-67]-[3-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-53]-[33-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-36]-[14-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-64]-[50-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-12]-[9-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-14]-[2-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-38]-[36-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-26]-[20-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-38]-[31-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-27]-[4-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-38]-[6-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-12]-[2-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-12]-[4-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-40]-[37-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-13]-[12-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-54]-[26-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-26]-[19-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-65]-[25-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-52]-[1-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-71]-[18-80]] --max_seq_len 8192 --api-port 5003, None
Qwen2-72B, ChatML, /data/llm_models/Qwen2-72B-Instruct-6.0bpw-h6-exl2, None, None, 1, ooba,  --loader exllamav2 --franken_layers [[0-42]-[38-80]] --max_seq_len 8192 --api-port 5003, None
